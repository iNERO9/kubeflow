{"kind":"Workflow","apiVersion":"argoproj.io/v1alpha1","metadata":{"generateName":"db-connection-forecasting-model-","creationTimestamp":null,"labels":{"pipelines.kubeflow.org/kfp_sdk_version":"1.8.13"},"annotations":{"pipelines.kubeflow.org/kfp_sdk_version":"1.8.13","pipelines.kubeflow.org/pipeline_compilation_time":"2023-03-16T14:18:27.406445","pipelines.kubeflow.org/pipeline_spec":"{\"description\": \"Kubeflow pipeline of DB Connection Forecast on RDS instances\", \"name\": \"DB_Connection_Forecasting_Model\"}"}},"spec":{"templates":[{"name":"calculating-threshold","inputs":{"parameters":[{"name":"loop-item-param-1-subvar-region"},{"name":"loop-item-param-1-subvar-resource_identifier"},{"name":"loop-item-param-1-subvar-resource_type"}],"artifacts":[{"name":"data-processing-result_modelled_ready_df","path":"/tmp/inputs/result_modelled_ready_df/data"}]},"outputs":{"parameters":[{"name":"calculating-threshold-Output","valueFrom":{"path":"/tmp/outputs/Output/data"}}],"artifacts":[{"name":"calculating-threshold-Output","path":"/tmp/outputs/Output/data"}]},"metadata":{"annotations":{"pipelines.kubeflow.org/arguments.parameters":"{\"region\": \"{{inputs.parameters.loop-item-param-1-subvar-region}}\", \"resource_identifier\": \"{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}\", \"resource_type\": \"{{inputs.parameters.loop-item-param-1-subvar-resource_type}}\"}","pipelines.kubeflow.org/component_ref":"{}","pipelines.kubeflow.org/component_spec":"{\"implementation\": {\"container\": {\"args\": [\"--result-modelled-ready-df\", {\"inputPath\": \"result_modelled_ready_df\"}, \"--region\", {\"inputValue\": \"region\"}, \"--resource-type\", {\"inputValue\": \"resource_type\"}, \"--resource-identifier\", {\"inputValue\": \"resource_identifier\"}, \"----output-paths\", {\"outputPath\": \"Output\"}], \"command\": [\"sh\", \"-c\", \"(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' --user) \u0026\u0026 \\\"$0\\\" \\\"$@\\\"\", \"sh\", \"-ec\", \"program_path=$(mktemp)\\nprintf \\\"%s\\\" \\\"$0\\\" \u003e \\\"$program_path\\\"\\npython3 -u \\\"$program_path\\\" \\\"$@\\\"\\n\", \"def Calculating_Threshold(result_modelled_ready_df,region,resource_type,resource_identifier): \\n    import seaborn as sns\\n    import pandas as pd\\n    import boto3\\n    import sys\\n    from io import StringIO\\n    import numpy as np\\n    from scipy import stats\\n    modelled_ready_df = pd.read_csv(result_modelled_ready_df)\\n    sns.boxplot(modelled_ready_df['CPUUtilization'],color='grey')\\n\\n    print('In the Dynamic threshold block')\\n    dynamic_threshold_df = modelled_ready_df.tail(12)\\n    THRESHOLD = round(2 * np.std(dynamic_threshold_df['CPUUtilization']) +  np.mean(dynamic_threshold_df['CPUUtilization']))\\n    print(\\\"Dynamic Threshold calculated for this run is ::-\u003e \\\",THRESHOLD)\\n    # print(f\\\"Sustained Avg Cpu Utilization for last 30 days is {avg_cpu_utilization}\\\")\\n    print(f\\\"Calculating Threshold on {region},{resource_type} and {resource_identifier} completed successfully\\\")\\n    return THRESHOLD\\n\\ndef _serialize_float(float_value: float) -\u003e str:\\n    if isinstance(float_value, str):\\n        return float_value\\n    if not isinstance(float_value, (float, int)):\\n        raise TypeError('Value \\\"{}\\\" has type \\\"{}\\\" instead of float.'.format(\\n            str(float_value), str(type(float_value))))\\n    return str(float_value)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog='Calculating Threshold', description='')\\n_parser.add_argument(\\\"--result-modelled-ready-df\\\", dest=\\\"result_modelled_ready_df\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--region\\\", dest=\\\"region\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--resource-type\\\", dest=\\\"resource_type\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--resource-identifier\\\", dest=\\\"resource_identifier\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"----output-paths\\\", dest=\\\"_output_paths\\\", type=str, nargs=1)\\n_parsed_args = vars(_parser.parse_args())\\n_output_files = _parsed_args.pop(\\\"_output_paths\\\", [])\\n\\n_outputs = Calculating_Threshold(**_parsed_args)\\n\\n_outputs = [_outputs]\\n\\n_output_serializers = [\\n    _serialize_float,\\n\\n]\\n\\nimport os\\nfor idx, output_file in enumerate(_output_files):\\n    try:\\n        os.makedirs(os.path.dirname(output_file))\\n    except OSError:\\n        pass\\n    with open(output_file, 'w') as f:\\n        f.write(_output_serializers[idx](_outputs[idx]))\\n\"], \"image\": \"python:3.7\"}}, \"inputs\": [{\"name\": \"result_modelled_ready_df\"}, {\"name\": \"region\"}, {\"name\": \"resource_type\"}, {\"name\": \"resource_identifier\"}], \"name\": \"Calculating Threshold\", \"outputs\": [{\"name\": \"Output\", \"type\": \"Float\"}]}","pipelines.kubeflow.org/max_cache_staleness":"P0D"},"labels":{"pipelines.kubeflow.org/enable_caching":"true","pipelines.kubeflow.org/kfp_sdk_version":"1.8.13","pipelines.kubeflow.org/pipeline-sdk-type":"kfp"}},"container":{"name":"","image":"python:3.7","command":["sh","-c","(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' --user) \u0026\u0026 \"$0\" \"$@\"","sh","-ec","program_path=$(mktemp)\nprintf \"%s\" \"$0\" \u003e \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n","def Calculating_Threshold(result_modelled_ready_df,region,resource_type,resource_identifier): \n    import seaborn as sns\n    import pandas as pd\n    import boto3\n    import sys\n    from io import StringIO\n    import numpy as np\n    from scipy import stats\n    modelled_ready_df = pd.read_csv(result_modelled_ready_df)\n    sns.boxplot(modelled_ready_df['CPUUtilization'],color='grey')\n\n    print('In the Dynamic threshold block')\n    dynamic_threshold_df = modelled_ready_df.tail(12)\n    THRESHOLD = round(2 * np.std(dynamic_threshold_df['CPUUtilization']) +  np.mean(dynamic_threshold_df['CPUUtilization']))\n    print(\"Dynamic Threshold calculated for this run is ::-\u003e \",THRESHOLD)\n    # print(f\"Sustained Avg Cpu Utilization for last 30 days is {avg_cpu_utilization}\")\n    print(f\"Calculating Threshold on {region},{resource_type} and {resource_identifier} completed successfully\")\n    return THRESHOLD\n\ndef _serialize_float(float_value: float) -\u003e str:\n    if isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value, (float, int)):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of float.'.format(\n            str(float_value), str(type(float_value))))\n    return str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Calculating Threshold', description='')\n_parser.add_argument(\"--result-modelled-ready-df\", dest=\"result_modelled_ready_df\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--region\", dest=\"region\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--resource-type\", dest=\"resource_type\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--resource-identifier\", dest=\"resource_identifier\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = Calculating_Threshold(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],"args":["--result-modelled-ready-df","/tmp/inputs/result_modelled_ready_df/data","--region","{{inputs.parameters.loop-item-param-1-subvar-region}}","--resource-type","{{inputs.parameters.loop-item-param-1-subvar-resource_type}}","--resource-identifier","{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}","----output-paths","/tmp/outputs/Output/data"],"resources":{}}},{"name":"condition-3","inputs":{},"outputs":{},"metadata":{},"dag":{"tasks":[{"name":"red-alert","template":"red-alert","arguments":{}}]}},{"name":"condition-4","inputs":{},"outputs":{},"metadata":{},"dag":{"tasks":[{"name":"green-alert","template":"green-alert","arguments":{}}]}},{"name":"data-fetching","inputs":{},"outputs":{"artifacts":[{"name":"data-fetching-result_ready_preprocessed","path":"/tmp/outputs/result_ready_preprocessed/data"}]},"metadata":{"annotations":{"pipelines.kubeflow.org/component_ref":"{}","pipelines.kubeflow.org/component_spec":"{\"implementation\": {\"container\": {\"args\": [\"--result-ready-preprocessed\", {\"outputPath\": \"result_ready_preprocessed\"}], \"command\": [\"sh\", \"-c\", \"(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' --user) \u0026\u0026 \\\"$0\\\" \\\"$@\\\"\", \"sh\", \"-ec\", \"program_path=$(mktemp)\\nprintf \\\"%s\\\" \\\"$0\\\" \u003e \\\"$program_path\\\"\\npython3 -u \\\"$program_path\\\" \\\"$@\\\"\\n\", \"def _make_parent_dirs_and_return_path(file_path: str):\\n    import os\\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n    return file_path\\n\\ndef Data_Fetching(result_ready_preprocessed):\\n\\n    import pandas as pd\\n    import boto3\\n    import sys\\n    from io import StringIO\\n    import warnings \\n\\n    import matplotlib.pyplot as plt\\n    import seaborn as sns\\n    from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\\n    import statsmodels.api as sm\\n    from statsmodels.tsa.arima.model import ARIMA\\n    import pmdarima as pm\\n    from sklearn.metrics import mean_absolute_error,mean_squared_error\\n    import datetime as dt\\n    import numpy as np\\n    from scipy import stats\\n\\n    # ignore all warnings issued by the pandas module\\n    warnings.filterwarnings(\\\"ignore\\\")\\n\\n    client = boto3.client('s3')\\n    bucket_name = 'lamm-prod-ml'\\n\\n#------------------------------------------------------------------------------------------ \\n    #Step 1 : ----------------Fetch Preprocessed file----------------\\n\\n    # object_key = 'preprocessed_data/All_region_rds_Real_Time_Preprocessed_metric_data_30_min.csv'\\n    object_key = 'preprocessed_data/All_rds_Real_Time_Preprocessed_metric_data_30_min - Copy.csv'\\n    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\\n    body = csv_obj['Body']\\n    csv_string = body.read().decode('utf-8')\\n    ready_preprocessed = pd.read_csv(StringIO(csv_string))\\n    ready_preprocessed.to_csv(result_ready_preprocessed)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog='Data Fetching', description='')\\n_parser.add_argument(\\\"--result-ready-preprocessed\\\", dest=\\\"result_ready_preprocessed\\\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parsed_args = vars(_parser.parse_args())\\n\\n_outputs = Data_Fetching(**_parsed_args)\\n\"], \"image\": \"python:3.7\"}}, \"name\": \"Data Fetching\", \"outputs\": [{\"name\": \"result_ready_preprocessed\"}]}","pipelines.kubeflow.org/max_cache_staleness":"P0D"},"labels":{"pipelines.kubeflow.org/enable_caching":"true","pipelines.kubeflow.org/kfp_sdk_version":"1.8.13","pipelines.kubeflow.org/pipeline-sdk-type":"kfp","rds-secret":"true"}},"container":{"name":"","image":"python:3.7","command":["sh","-c","(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' --user) \u0026\u0026 \"$0\" \"$@\"","sh","-ec","program_path=$(mktemp)\nprintf \"%s\" \"$0\" \u003e \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n","def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\ndef Data_Fetching(result_ready_preprocessed):\n\n    import pandas as pd\n    import boto3\n    import sys\n    from io import StringIO\n    import warnings \n\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n    import statsmodels.api as sm\n    from statsmodels.tsa.arima.model import ARIMA\n    import pmdarima as pm\n    from sklearn.metrics import mean_absolute_error,mean_squared_error\n    import datetime as dt\n    import numpy as np\n    from scipy import stats\n\n    # ignore all warnings issued by the pandas module\n    warnings.filterwarnings(\"ignore\")\n\n    client = boto3.client('s3')\n    bucket_name = 'lamm-prod-ml'\n\n#------------------------------------------------------------------------------------------ \n    #Step 1 : ----------------Fetch Preprocessed file----------------\n\n    # object_key = 'preprocessed_data/All_region_rds_Real_Time_Preprocessed_metric_data_30_min.csv'\n    object_key = 'preprocessed_data/All_rds_Real_Time_Preprocessed_metric_data_30_min - Copy.csv'\n    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n    body = csv_obj['Body']\n    csv_string = body.read().decode('utf-8')\n    ready_preprocessed = pd.read_csv(StringIO(csv_string))\n    ready_preprocessed.to_csv(result_ready_preprocessed)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Data Fetching', description='')\n_parser.add_argument(\"--result-ready-preprocessed\", dest=\"result_ready_preprocessed\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs = Data_Fetching(**_parsed_args)\n"],"args":["--result-ready-preprocessed","/tmp/outputs/result_ready_preprocessed/data"],"resources":{}}},{"name":"data-processing","inputs":{"parameters":[{"name":"loop-item-param-1-subvar-region"},{"name":"loop-item-param-1-subvar-resource_identifier"},{"name":"loop-item-param-1-subvar-resource_type"}],"artifacts":[{"name":"data-fetching-result_ready_preprocessed","path":"/tmp/inputs/result_ready_preprocessed/data"}]},"outputs":{"artifacts":[{"name":"data-processing-result_modelled_ready_df","path":"/tmp/outputs/result_modelled_ready_df/data"}]},"metadata":{"annotations":{"pipelines.kubeflow.org/arguments.parameters":"{\"region\": \"{{inputs.parameters.loop-item-param-1-subvar-region}}\", \"resource_identifier\": \"{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}\", \"resource_type\": \"{{inputs.parameters.loop-item-param-1-subvar-resource_type}}\"}","pipelines.kubeflow.org/component_ref":"{}","pipelines.kubeflow.org/component_spec":"{\"implementation\": {\"container\": {\"args\": [\"--result-ready-preprocessed\", {\"inputPath\": \"result_ready_preprocessed\"}, \"--region\", {\"inputValue\": \"region\"}, \"--resource-type\", {\"inputValue\": \"resource_type\"}, \"--resource-identifier\", {\"inputValue\": \"resource_identifier\"}, \"--result-modelled-ready-df\", {\"outputPath\": \"result_modelled_ready_df\"}], \"command\": [\"sh\", \"-c\", \"(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' --user) \u0026\u0026 \\\"$0\\\" \\\"$@\\\"\", \"sh\", \"-ec\", \"program_path=$(mktemp)\\nprintf \\\"%s\\\" \\\"$0\\\" \u003e \\\"$program_path\\\"\\npython3 -u \\\"$program_path\\\" \\\"$@\\\"\\n\", \"def _make_parent_dirs_and_return_path(file_path: str):\\n    import os\\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n    return file_path\\n\\ndef Data_Processing(result_ready_preprocessed,region,resource_type, resource_identifier,result_modelled_ready_df):\\n\\n    import pandas as pd\\n    import boto3\\n    import sys\\n    from io import StringIO\\n    import warnings \\n\\n    import matplotlib.pyplot as plt\\n    import seaborn as sns\\n    from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\\n    import statsmodels.api as sm\\n    from statsmodels.tsa.arima.model import ARIMA\\n    import pmdarima as pm\\n    from sklearn.metrics import mean_absolute_error,mean_squared_error\\n    import datetime as dt\\n    import numpy as np\\n    from scipy import stats\\n\\n    results = pd.read_csv(result_ready_preprocessed)\\n    ready_preprocessed = results\\n    datum = ready_preprocessed\\n    processing_datum = datum.loc[(datum['region'] == region) \u0026 (datum['resource_type'] == resource_type) \u0026 (datum['resource_identifier'] == resource_identifier)]\\n    processing_datum_wod = processing_datum.drop_duplicates()\\n    processing_datum_wod.sort_values(by='sample_utc',inplace=True)\\n    unique_metrics_list = list(processing_datum_wod[\\\"metric_name\\\"].explode().unique())\\n    list_of_modelled_variables = ['sample_utc','resource_type','resource_identifier','metric_name','sample_max']\\n    processing_datum_wod_imp_variables = processing_datum_wod[list_of_modelled_variables]\\n    processed_datum = pd.DataFrame()\\n\\n    for i in range(len(unique_metrics_list)):\\n        datum_temp = processing_datum_wod_imp_variables.loc[processing_datum['metric_name'] == unique_metrics_list[i]]\\n        datum_temp[unique_metrics_list[i]] = datum_temp['sample_max']\\n        datum_temp = datum_temp[['sample_utc',unique_metrics_list[i]]]\\n        if i ==0:\\n            processed_datum = datum_temp\\n            print(processed_datum)\\n        else:\\n            processed_datum = pd.merge(processed_datum,datum_temp,on='sample_utc')\\n\\n    # processed_datum['sample_utc'] = pd.to_datetime(processed_datum['sample_utc']).dt.date\\n    processed_datum['sample_utc'] = processed_datum['sample_utc'].astype(\\\"datetime64\\\")\\n    processed_datum = processed_datum.set_index(\\\"sample_utc\\\")\\n    print(f\\\"starting date of the processed data : {str(processed_datum.index.min())}\\\")\\n    print(f\\\"end date of the proccessed data : {str(processed_datum.index.max())}\\\")  \\n    modelling_dataframe = processed_datum[['CPUUtilization']]\\n    modelled_ready_df = modelling_dataframe.reset_index()\\n    print('Model Ready Dataframe use for modelling')\\n    modelled_ready_df = modelled_ready_df.loc[~modelled_ready_df.index.duplicated(), :]\\n    modelled_ready_df = modelled_ready_df.drop_duplicates(keep='first')\\n\\n    #modelled_ready_df.to_csv(rds_preprocessed_formatted)\\n    print(f\\\"Filtering on {region}, {resource_type} and {resource_identifier} and reformatting completed successfully\\\")\\n\\n    modelled_ready_df.to_csv(result_modelled_ready_df)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog='Data Processing', description='')\\n_parser.add_argument(\\\"--result-ready-preprocessed\\\", dest=\\\"result_ready_preprocessed\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--region\\\", dest=\\\"region\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--resource-type\\\", dest=\\\"resource_type\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--resource-identifier\\\", dest=\\\"resource_identifier\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--result-modelled-ready-df\\\", dest=\\\"result_modelled_ready_df\\\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parsed_args = vars(_parser.parse_args())\\n\\n_outputs = Data_Processing(**_parsed_args)\\n\"], \"image\": \"python:3.7\"}}, \"inputs\": [{\"name\": \"result_ready_preprocessed\"}, {\"name\": \"region\"}, {\"name\": \"resource_type\"}, {\"name\": \"resource_identifier\"}], \"name\": \"Data Processing\", \"outputs\": [{\"name\": \"result_modelled_ready_df\"}]}","pipelines.kubeflow.org/max_cache_staleness":"P0D"},"labels":{"pipelines.kubeflow.org/enable_caching":"true","pipelines.kubeflow.org/kfp_sdk_version":"1.8.13","pipelines.kubeflow.org/pipeline-sdk-type":"kfp"}},"container":{"name":"","image":"python:3.7","command":["sh","-c","(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' --user) \u0026\u0026 \"$0\" \"$@\"","sh","-ec","program_path=$(mktemp)\nprintf \"%s\" \"$0\" \u003e \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n","def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\ndef Data_Processing(result_ready_preprocessed,region,resource_type, resource_identifier,result_modelled_ready_df):\n\n    import pandas as pd\n    import boto3\n    import sys\n    from io import StringIO\n    import warnings \n\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n    import statsmodels.api as sm\n    from statsmodels.tsa.arima.model import ARIMA\n    import pmdarima as pm\n    from sklearn.metrics import mean_absolute_error,mean_squared_error\n    import datetime as dt\n    import numpy as np\n    from scipy import stats\n\n    results = pd.read_csv(result_ready_preprocessed)\n    ready_preprocessed = results\n    datum = ready_preprocessed\n    processing_datum = datum.loc[(datum['region'] == region) \u0026 (datum['resource_type'] == resource_type) \u0026 (datum['resource_identifier'] == resource_identifier)]\n    processing_datum_wod = processing_datum.drop_duplicates()\n    processing_datum_wod.sort_values(by='sample_utc',inplace=True)\n    unique_metrics_list = list(processing_datum_wod[\"metric_name\"].explode().unique())\n    list_of_modelled_variables = ['sample_utc','resource_type','resource_identifier','metric_name','sample_max']\n    processing_datum_wod_imp_variables = processing_datum_wod[list_of_modelled_variables]\n    processed_datum = pd.DataFrame()\n\n    for i in range(len(unique_metrics_list)):\n        datum_temp = processing_datum_wod_imp_variables.loc[processing_datum['metric_name'] == unique_metrics_list[i]]\n        datum_temp[unique_metrics_list[i]] = datum_temp['sample_max']\n        datum_temp = datum_temp[['sample_utc',unique_metrics_list[i]]]\n        if i ==0:\n            processed_datum = datum_temp\n            print(processed_datum)\n        else:\n            processed_datum = pd.merge(processed_datum,datum_temp,on='sample_utc')\n\n    # processed_datum['sample_utc'] = pd.to_datetime(processed_datum['sample_utc']).dt.date\n    processed_datum['sample_utc'] = processed_datum['sample_utc'].astype(\"datetime64\")\n    processed_datum = processed_datum.set_index(\"sample_utc\")\n    print(f\"starting date of the processed data : {str(processed_datum.index.min())}\")\n    print(f\"end date of the proccessed data : {str(processed_datum.index.max())}\")  \n    modelling_dataframe = processed_datum[['CPUUtilization']]\n    modelled_ready_df = modelling_dataframe.reset_index()\n    print('Model Ready Dataframe use for modelling')\n    modelled_ready_df = modelled_ready_df.loc[~modelled_ready_df.index.duplicated(), :]\n    modelled_ready_df = modelled_ready_df.drop_duplicates(keep='first')\n\n    #modelled_ready_df.to_csv(rds_preprocessed_formatted)\n    print(f\"Filtering on {region}, {resource_type} and {resource_identifier} and reformatting completed successfully\")\n\n    modelled_ready_df.to_csv(result_modelled_ready_df)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Data Processing', description='')\n_parser.add_argument(\"--result-ready-preprocessed\", dest=\"result_ready_preprocessed\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--region\", dest=\"region\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--resource-type\", dest=\"resource_type\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--resource-identifier\", dest=\"resource_identifier\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--result-modelled-ready-df\", dest=\"result_modelled_ready_df\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs = Data_Processing(**_parsed_args)\n"],"args":["--result-ready-preprocessed","/tmp/inputs/result_ready_preprocessed/data","--region","{{inputs.parameters.loop-item-param-1-subvar-region}}","--resource-type","{{inputs.parameters.loop-item-param-1-subvar-resource_type}}","--resource-identifier","{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}","--result-modelled-ready-df","/tmp/outputs/result_modelled_ready_df/data"],"resources":{}}},{"name":"db-connection-forecasting-model","inputs":{},"outputs":{},"metadata":{},"dag":{"tasks":[{"name":"data-fetching","template":"data-fetching","arguments":{}},{"name":"for-loop-2","template":"for-loop-2","arguments":{"parameters":[{"name":"loop-item-param-1-subvar-region","value":"{{item.region}}"},{"name":"loop-item-param-1-subvar-resource_identifier","value":"{{item.resource_identifier}}"},{"name":"loop-item-param-1-subvar-resource_type","value":"{{item.resource_type}}"}],"artifacts":[{"name":"data-fetching-result_ready_preprocessed","from":"{{tasks.data-fetching.outputs.artifacts.data-fetching-result_ready_preprocessed}}"}]},"dependencies":["data-fetching"],"withItems":[{"region":"us-east-1","resource_identifier":"cihdb-prod-usw5-aurora-dr-cluster-1","resource_type":"DBClusterIdentifier"},{"region":"us-east-1","resource_identifier":"cihdb-use2-prod-aurora","resource_type":"DBClusterIdentifier"}]},{"name":"rds-metrics","template":"rds-metrics","arguments":{"parameters":[{"name":"send-alerts-Output","value":"{{tasks.for-loop-2.outputs.parameters.send-alerts-Output}}"}]},"dependencies":["for-loop-2"]}]}},{"name":"for-loop-2","inputs":{"parameters":[{"name":"loop-item-param-1-subvar-region"},{"name":"loop-item-param-1-subvar-resource_identifier"},{"name":"loop-item-param-1-subvar-resource_type"}],"artifacts":[{"name":"data-fetching-result_ready_preprocessed"}]},"outputs":{"parameters":[{"name":"send-alerts-Output","valueFrom":{"parameter":"{{tasks.send-alerts.outputs.parameters.send-alerts-Output}}"}}]},"metadata":{},"dag":{"tasks":[{"name":"calculating-threshold","template":"calculating-threshold","arguments":{"parameters":[{"name":"loop-item-param-1-subvar-region","value":"{{inputs.parameters.loop-item-param-1-subvar-region}}"},{"name":"loop-item-param-1-subvar-resource_identifier","value":"{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}"},{"name":"loop-item-param-1-subvar-resource_type","value":"{{inputs.parameters.loop-item-param-1-subvar-resource_type}}"}],"artifacts":[{"name":"data-processing-result_modelled_ready_df","from":"{{tasks.data-processing.outputs.artifacts.data-processing-result_modelled_ready_df}}"}]},"dependencies":["data-processing"]},{"name":"condition-3","template":"condition-3","arguments":{},"dependencies":["send-alerts"],"when":"\"{{tasks.send-alerts.outputs.parameters.send-alerts-Output}}\" == \"Red Alert\""},{"name":"condition-4","template":"condition-4","arguments":{},"dependencies":["send-alerts"],"when":"\"{{tasks.send-alerts.outputs.parameters.send-alerts-Output}}\" == \"Green Alert\""},{"name":"data-processing","template":"data-processing","arguments":{"parameters":[{"name":"loop-item-param-1-subvar-region","value":"{{inputs.parameters.loop-item-param-1-subvar-region}}"},{"name":"loop-item-param-1-subvar-resource_identifier","value":"{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}"},{"name":"loop-item-param-1-subvar-resource_type","value":"{{inputs.parameters.loop-item-param-1-subvar-resource_type}}"}],"artifacts":[{"name":"data-fetching-result_ready_preprocessed","from":"{{inputs.artifacts.data-fetching-result_ready_preprocessed}}"}]}},{"name":"forecast-model","template":"forecast-model","arguments":{"parameters":[{"name":"calculating-threshold-Output","value":"{{tasks.calculating-threshold.outputs.parameters.calculating-threshold-Output}}"},{"name":"loop-item-param-1-subvar-region","value":"{{inputs.parameters.loop-item-param-1-subvar-region}}"},{"name":"loop-item-param-1-subvar-resource_identifier","value":"{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}"},{"name":"loop-item-param-1-subvar-resource_type","value":"{{inputs.parameters.loop-item-param-1-subvar-resource_type}}"}],"artifacts":[{"name":"data-processing-result_modelled_ready_df","from":"{{tasks.data-processing.outputs.artifacts.data-processing-result_modelled_ready_df}}"}]},"dependencies":["calculating-threshold","data-processing"]},{"name":"send-alerts","template":"send-alerts","arguments":{"parameters":[{"name":"calculating-threshold-Output","value":"{{tasks.calculating-threshold.outputs.parameters.calculating-threshold-Output}}"},{"name":"loop-item-param-1-subvar-region","value":"{{inputs.parameters.loop-item-param-1-subvar-region}}"},{"name":"loop-item-param-1-subvar-resource_identifier","value":"{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}"},{"name":"loop-item-param-1-subvar-resource_type","value":"{{inputs.parameters.loop-item-param-1-subvar-resource_type}}"}],"artifacts":[{"name":"forecast-model-result_new_forecast_df","from":"{{tasks.forecast-model.outputs.artifacts.forecast-model-result_new_forecast_df}}"},{"name":"forecast-model-results","from":"{{tasks.forecast-model.outputs.artifacts.forecast-model-results}}"}]},"dependencies":["calculating-threshold","forecast-model","storing-forecasted-values"]},{"name":"storing-forecasted-values","template":"storing-forecasted-values","arguments":{"parameters":[{"name":"calculating-threshold-Output","value":"{{tasks.calculating-threshold.outputs.parameters.calculating-threshold-Output}}"},{"name":"loop-item-param-1-subvar-region","value":"{{inputs.parameters.loop-item-param-1-subvar-region}}"},{"name":"loop-item-param-1-subvar-resource_identifier","value":"{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}"},{"name":"loop-item-param-1-subvar-resource_type","value":"{{inputs.parameters.loop-item-param-1-subvar-resource_type}}"}],"artifacts":[{"name":"forecast-model-result_new_forecast_df","from":"{{tasks.forecast-model.outputs.artifacts.forecast-model-result_new_forecast_df}}"},{"name":"forecast-model-results","from":"{{tasks.forecast-model.outputs.artifacts.forecast-model-results}}"}]},"dependencies":["calculating-threshold","forecast-model"]}]},"parallelism":1000},{"name":"forecast-model","inputs":{"parameters":[{"name":"calculating-threshold-Output"},{"name":"loop-item-param-1-subvar-region"},{"name":"loop-item-param-1-subvar-resource_identifier"},{"name":"loop-item-param-1-subvar-resource_type"}],"artifacts":[{"name":"data-processing-result_modelled_ready_df","path":"/tmp/inputs/result_modelled_ready_df/data"}]},"outputs":{"artifacts":[{"name":"forecast-model-result_new_forecast_df","path":"/tmp/outputs/result_new_forecast_df/data"},{"name":"forecast-model-results","path":"/tmp/outputs/results/data"}]},"metadata":{"annotations":{"pipelines.kubeflow.org/arguments.parameters":"{\"THRESHOLD\": \"{{inputs.parameters.calculating-threshold-Output}}\", \"region\": \"{{inputs.parameters.loop-item-param-1-subvar-region}}\", \"resource_identifier\": \"{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}\", \"resource_type\": \"{{inputs.parameters.loop-item-param-1-subvar-resource_type}}\"}","pipelines.kubeflow.org/component_ref":"{}","pipelines.kubeflow.org/component_spec":"{\"implementation\": {\"container\": {\"args\": [\"--result-modelled-ready-df\", {\"inputPath\": \"result_modelled_ready_df\"}, \"--THRESHOLD\", {\"inputValue\": \"THRESHOLD\"}, \"--region\", {\"inputValue\": \"region\"}, \"--resource-type\", {\"inputValue\": \"resource_type\"}, \"--resource-identifier\", {\"inputValue\": \"resource_identifier\"}, \"--results\", {\"outputPath\": \"results\"}, \"--result-new-forecast-df\", {\"outputPath\": \"result_new_forecast_df\"}], \"command\": [\"sh\", \"-c\", \"(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' --user) \u0026\u0026 \\\"$0\\\" \\\"$@\\\"\", \"sh\", \"-ec\", \"program_path=$(mktemp)\\nprintf \\\"%s\\\" \\\"$0\\\" \u003e \\\"$program_path\\\"\\npython3 -u \\\"$program_path\\\" \\\"$@\\\"\\n\", \"def _make_parent_dirs_and_return_path(file_path: str):\\n    import os\\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n    return file_path\\n\\ndef Forecast_model(result_modelled_ready_df,THRESHOLD,results,result_new_forecast_df,region,resource_type,resource_identifier):\\n    import pandas as pd\\n    from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\\n    import statsmodels.api as sm\\n    import matplotlib.pyplot as plt\\n    import seaborn as sns\\n    import statsmodels.api as sm\\n    #from prophet import Prophet\\n    from sklearn.metrics import mean_absolute_error\\n    from sklearn.metrics import mean_squared_error\\n    import boto3\\n    import pandas as pd\\n    import sys\\n    from io import StringIO\\n    import pandas as pd\\n    from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\\n    import statsmodels.api as sm\\n    import matplotlib.pyplot as plt\\n    import seaborn as sns\\n    import statsmodels.api as sm\\n    from statsmodels.tsa.arima.model import ARIMA\\n    import seaborn as sns\\n    import pandas as pd\\n    import boto3\\n    import pandas as pd\\n    import sys\\n    from io import StringIO\\n    import pmdarima as pm\\n\\n    modelled_ready_df = pd.read_csv(result_modelled_ready_df)\\n    #modelled_ready_df = pd.read_csv(rds_preprocessed_formatted)\\n\\n    #modelled_ready_df = modelled_ready_df.set_index('sample_utc')\\n    modelled_ready_df = modelled_ready_df[['sample_utc','CPUUtilization']]\\n    modelled_ready_df = modelled_ready_df.set_index('sample_utc')\\n\\n    model = pm.auto_arima(modelled_ready_df['CPUUtilization'], seasonal=True,\\n                      start_p=0, start_q=0, max_order=8, test='adf',error_action='ignore',  \\n                      suppress_warnings=True,\\n                      stepwise=True, trace=True)\\n\\n    model.summary()\\n\\n    forecast=model.predict(n_periods=1, return_conf_int=True)\\n    f = list(forecast[0])\\n    print(\\\"Forecast\\\",f[0])\\n\\n    new_forecast_df = pd.DataFrame(f,columns=['Prediction'])\\n    forecast_term = resource_identifier+'_Forecasted'\\n    threshold_term = resource_identifier+'_Threshold'\\n    forecast_df = pd.DataFrame(f,columns=[forecast_term])\\n\\n    data_tail = modelled_ready_df.tail(1)\\n    print(\\\"Data Tail:\\\",data_tail)\\n    last_date = data_tail.index[-1]\\n    print(\\\"Last date time \\\",last_date)\\n    # data_tail_new = date_df['sample_utc'].tail(1)\\n    # try:\\n    #     try:\\n    #         try:\\n    #             last_date1 = data_tail.index[1]\\n    #             print(\\\"Last date time 1\\\",last_date1)\\n    #         except:\\n    #             last_date2 = data_tail.index[-1]\\n    #             print(\\\"Last date time 2\\\",last_date2)\\n    #     except:\\n    #         last_date3 = modelled_ready_df.index[-1]\\n    #         print(\\\"Last date time 3\\\",last_date3)\\n    # except:\\n    #     pass\\n\\n    today_actual = modelled_ready_df['CPUUtilization'][last_date]\\n    print(\\\"Previous 30 Min Actual Value\\\",today_actual)\\n    # Convert the date string to a datetime object\\n    date_obj = pd.to_datetime(last_date)\\n    # Increment the date by one day\\n    incremented_date = date_obj + pd.DateOffset(minutes=30)\\n    print(\\\"Next 30 Min date time\\\",incremented_date)\\n\\n    forecast_df['date'] = incremented_date\\n    print(\\\"Forecast_df:\\\",forecast_df['date'])\\n\\n    #******\\n    new_forecast_df['Date'] = incremented_date\\n    print(new_forecast_df['Date'])\\n    new_forecast_df = new_forecast_df[['Date','Prediction']]\\n\\n    print(\\\"*******\\\")\\n    if 'Unnamed: 0' in new_forecast_df:\\n        new_forecast_df = new_forecast_df.drop(['Unnamed: 0'], axis = 1)\\n        new_forecast_df = new_forecast_df.reset_index(drop=True)\\n\\n    else:\\n        pass\\n\\n    print(\\\"*******\\\")\\n\\n    print(new_forecast_df)\\n    #******\\n\\n    # Please ensure to keep this\\n    # forecast_df = forecast_df[['Date','Prediction']]\\n    # end\\n   # forecast_df.to_csv(forecasted_value)\\n\\n    #*******************************************************Monitoring***********************\\n    # forecast_df['date'] = incremented_date\\n    # forecast_term = resource_identifier+'_forecasted'\\n    # forecast_df['THRESHOLD'] = THRESHOLD\\n    # forecast_df = forecast_df[['date',forecast_term,'THRESHOLD']]\\n\\n    #---\\n    # forecast_df = forecast_df[['date',forecast_term]]\\n    # forecast_df.to_csv(forecasted_value)\\n    next_day_forecast=f[0]\\n    THRESHOLD = THRESHOLD\\n    # today_date = modelled_ready_df['sample_utc'].tail(1)\\n    today_date = pd.to_datetime(last_date)\\n    print(\\\"Last data point datetime\\\", today_date) \\n    #*******************************************************Monitoring***********************\\n\\n    # new_forecast_df.to_csv(forecasted_value)\\n    print(f\\\"Successfully build forecasted model for {region} on {resource_type} and {resource_identifier}\\\")\\n\\n    dict1 = {'Incremented Date':[incremented_date],'Today Date':[today_date],'Today Actual':[today_actual],'Next Day Forecast':[next_day_forecast]}\\n    print(\\\"dict1\\\",dict1)\\n    results1 = pd.DataFrame(dict1)\\n    results1.to_csv(results)\\n\\n    new_forecast_df.to_csv(result_new_forecast_df)\\n\\n    # return incremented_date,today_date,new_forecast_df,f,today_actual\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog='Forecast model', description='')\\n_parser.add_argument(\\\"--result-modelled-ready-df\\\", dest=\\\"result_modelled_ready_df\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--THRESHOLD\\\", dest=\\\"THRESHOLD\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--region\\\", dest=\\\"region\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--resource-type\\\", dest=\\\"resource_type\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--resource-identifier\\\", dest=\\\"resource_identifier\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--results\\\", dest=\\\"results\\\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--result-new-forecast-df\\\", dest=\\\"result_new_forecast_df\\\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parsed_args = vars(_parser.parse_args())\\n\\n_outputs = Forecast_model(**_parsed_args)\\n\"], \"image\": \"python:3.7\"}}, \"inputs\": [{\"name\": \"result_modelled_ready_df\"}, {\"name\": \"THRESHOLD\"}, {\"name\": \"region\"}, {\"name\": \"resource_type\"}, {\"name\": \"resource_identifier\"}], \"name\": \"Forecast model\", \"outputs\": [{\"name\": \"results\"}, {\"name\": \"result_new_forecast_df\"}]}","pipelines.kubeflow.org/max_cache_staleness":"P0D"},"labels":{"pipelines.kubeflow.org/enable_caching":"true","pipelines.kubeflow.org/kfp_sdk_version":"1.8.13","pipelines.kubeflow.org/pipeline-sdk-type":"kfp"}},"container":{"name":"","image":"python:3.7","command":["sh","-c","(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' --user) \u0026\u0026 \"$0\" \"$@\"","sh","-ec","program_path=$(mktemp)\nprintf \"%s\" \"$0\" \u003e \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n","def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\ndef Forecast_model(result_modelled_ready_df,THRESHOLD,results,result_new_forecast_df,region,resource_type,resource_identifier):\n    import pandas as pd\n    from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n    import statsmodels.api as sm\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import statsmodels.api as sm\n    #from prophet import Prophet\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import mean_squared_error\n    import boto3\n    import pandas as pd\n    import sys\n    from io import StringIO\n    import pandas as pd\n    from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n    import statsmodels.api as sm\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import statsmodels.api as sm\n    from statsmodels.tsa.arima.model import ARIMA\n    import seaborn as sns\n    import pandas as pd\n    import boto3\n    import pandas as pd\n    import sys\n    from io import StringIO\n    import pmdarima as pm\n\n    modelled_ready_df = pd.read_csv(result_modelled_ready_df)\n    #modelled_ready_df = pd.read_csv(rds_preprocessed_formatted)\n\n    #modelled_ready_df = modelled_ready_df.set_index('sample_utc')\n    modelled_ready_df = modelled_ready_df[['sample_utc','CPUUtilization']]\n    modelled_ready_df = modelled_ready_df.set_index('sample_utc')\n\n    model = pm.auto_arima(modelled_ready_df['CPUUtilization'], seasonal=True,\n                      start_p=0, start_q=0, max_order=8, test='adf',error_action='ignore',  \n                      suppress_warnings=True,\n                      stepwise=True, trace=True)\n\n    model.summary()\n\n    forecast=model.predict(n_periods=1, return_conf_int=True)\n    f = list(forecast[0])\n    print(\"Forecast\",f[0])\n\n    new_forecast_df = pd.DataFrame(f,columns=['Prediction'])\n    forecast_term = resource_identifier+'_Forecasted'\n    threshold_term = resource_identifier+'_Threshold'\n    forecast_df = pd.DataFrame(f,columns=[forecast_term])\n\n    data_tail = modelled_ready_df.tail(1)\n    print(\"Data Tail:\",data_tail)\n    last_date = data_tail.index[-1]\n    print(\"Last date time \",last_date)\n    # data_tail_new = date_df['sample_utc'].tail(1)\n    # try:\n    #     try:\n    #         try:\n    #             last_date1 = data_tail.index[1]\n    #             print(\"Last date time 1\",last_date1)\n    #         except:\n    #             last_date2 = data_tail.index[-1]\n    #             print(\"Last date time 2\",last_date2)\n    #     except:\n    #         last_date3 = modelled_ready_df.index[-1]\n    #         print(\"Last date time 3\",last_date3)\n    # except:\n    #     pass\n\n    today_actual = modelled_ready_df['CPUUtilization'][last_date]\n    print(\"Previous 30 Min Actual Value\",today_actual)\n    # Convert the date string to a datetime object\n    date_obj = pd.to_datetime(last_date)\n    # Increment the date by one day\n    incremented_date = date_obj + pd.DateOffset(minutes=30)\n    print(\"Next 30 Min date time\",incremented_date)\n\n    forecast_df['date'] = incremented_date\n    print(\"Forecast_df:\",forecast_df['date'])\n\n    #******\n    new_forecast_df['Date'] = incremented_date\n    print(new_forecast_df['Date'])\n    new_forecast_df = new_forecast_df[['Date','Prediction']]\n\n    print(\"*******\")\n    if 'Unnamed: 0' in new_forecast_df:\n        new_forecast_df = new_forecast_df.drop(['Unnamed: 0'], axis = 1)\n        new_forecast_df = new_forecast_df.reset_index(drop=True)\n\n    else:\n        pass\n\n    print(\"*******\")\n\n    print(new_forecast_df)\n    #******\n\n    # Please ensure to keep this\n    # forecast_df = forecast_df[['Date','Prediction']]\n    # end\n   # forecast_df.to_csv(forecasted_value)\n\n    #*******************************************************Monitoring***********************\n    # forecast_df['date'] = incremented_date\n    # forecast_term = resource_identifier+'_forecasted'\n    # forecast_df['THRESHOLD'] = THRESHOLD\n    # forecast_df = forecast_df[['date',forecast_term,'THRESHOLD']]\n\n    #---\n    # forecast_df = forecast_df[['date',forecast_term]]\n    # forecast_df.to_csv(forecasted_value)\n    next_day_forecast=f[0]\n    THRESHOLD = THRESHOLD\n    # today_date = modelled_ready_df['sample_utc'].tail(1)\n    today_date = pd.to_datetime(last_date)\n    print(\"Last data point datetime\", today_date) \n    #*******************************************************Monitoring***********************\n\n    # new_forecast_df.to_csv(forecasted_value)\n    print(f\"Successfully build forecasted model for {region} on {resource_type} and {resource_identifier}\")\n\n    dict1 = {'Incremented Date':[incremented_date],'Today Date':[today_date],'Today Actual':[today_actual],'Next Day Forecast':[next_day_forecast]}\n    print(\"dict1\",dict1)\n    results1 = pd.DataFrame(dict1)\n    results1.to_csv(results)\n\n    new_forecast_df.to_csv(result_new_forecast_df)\n\n    # return incremented_date,today_date,new_forecast_df,f,today_actual\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Forecast model', description='')\n_parser.add_argument(\"--result-modelled-ready-df\", dest=\"result_modelled_ready_df\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--THRESHOLD\", dest=\"THRESHOLD\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--region\", dest=\"region\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--resource-type\", dest=\"resource_type\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--resource-identifier\", dest=\"resource_identifier\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--results\", dest=\"results\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--result-new-forecast-df\", dest=\"result_new_forecast_df\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs = Forecast_model(**_parsed_args)\n"],"args":["--result-modelled-ready-df","/tmp/inputs/result_modelled_ready_df/data","--THRESHOLD","{{inputs.parameters.calculating-threshold-Output}}","--region","{{inputs.parameters.loop-item-param-1-subvar-region}}","--resource-type","{{inputs.parameters.loop-item-param-1-subvar-resource_type}}","--resource-identifier","{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}","--results","/tmp/outputs/results/data","--result-new-forecast-df","/tmp/outputs/result_new_forecast_df/data"],"resources":{}}},{"name":"green-alert","inputs":{},"outputs":{"artifacts":[{"name":"green-alert-Output","path":"/tmp/outputs/Output/data"}]},"metadata":{"annotations":{"pipelines.kubeflow.org/arguments.parameters":"{\"text1\": \"Green Alert\"}","pipelines.kubeflow.org/component_ref":"{}","pipelines.kubeflow.org/component_spec":"{\"implementation\": {\"container\": {\"args\": [\"--text1\", {\"inputValue\": \"text1\"}, \"----output-paths\", {\"outputPath\": \"Output\"}], \"command\": [\"sh\", \"-ec\", \"program_path=$(mktemp)\\nprintf \\\"%s\\\" \\\"$0\\\" \u003e \\\"$program_path\\\"\\npython3 -u \\\"$program_path\\\" \\\"$@\\\"\\n\", \"def GREEN_ALERT(text1):\\n    print('echo \\\"Results:{} \\\"'.format(text1))\\n    print(\\\"text\\\",text1)\\n\\n    return text1\\n\\ndef _serialize_str(str_value: str) -\u003e str:\\n    if not isinstance(str_value, str):\\n        raise TypeError('Value \\\"{}\\\" has type \\\"{}\\\" instead of str.'.format(\\n            str(str_value), str(type(str_value))))\\n    return str_value\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog='GREEN ALERT', description='')\\n_parser.add_argument(\\\"--text1\\\", dest=\\\"text1\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"----output-paths\\\", dest=\\\"_output_paths\\\", type=str, nargs=1)\\n_parsed_args = vars(_parser.parse_args())\\n_output_files = _parsed_args.pop(\\\"_output_paths\\\", [])\\n\\n_outputs = GREEN_ALERT(**_parsed_args)\\n\\n_outputs = [_outputs]\\n\\n_output_serializers = [\\n    _serialize_str,\\n\\n]\\n\\nimport os\\nfor idx, output_file in enumerate(_output_files):\\n    try:\\n        os.makedirs(os.path.dirname(output_file))\\n    except OSError:\\n        pass\\n    with open(output_file, 'w') as f:\\n        f.write(_output_serializers[idx](_outputs[idx]))\\n\"], \"image\": \"python:3.7\"}}, \"inputs\": [{\"name\": \"text1\", \"type\": \"String\"}], \"name\": \"GREEN ALERT\", \"outputs\": [{\"name\": \"Output\", \"type\": \"String\"}]}","pipelines.kubeflow.org/max_cache_staleness":"P0D"},"labels":{"pipelines.kubeflow.org/enable_caching":"true","pipelines.kubeflow.org/kfp_sdk_version":"1.8.13","pipelines.kubeflow.org/pipeline-sdk-type":"kfp"}},"container":{"name":"","image":"python:3.7","command":["sh","-ec","program_path=$(mktemp)\nprintf \"%s\" \"$0\" \u003e \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n","def GREEN_ALERT(text1):\n    print('echo \"Results:{} \"'.format(text1))\n    print(\"text\",text1)\n\n    return text1\n\ndef _serialize_str(str_value: str) -\u003e str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value), str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='GREEN ALERT', description='')\n_parser.add_argument(\"--text1\", dest=\"text1\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = GREEN_ALERT(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],"args":["--text1","Green Alert","----output-paths","/tmp/outputs/Output/data"],"resources":{}}},{"name":"rds-metrics","inputs":{"parameters":[{"name":"send-alerts-Output"}]},"outputs":{"artifacts":[{"name":"mlpipeline-metrics","path":"/tmp/outputs/mlpipeline_metrics/data"}]},"metadata":{"annotations":{"pipelines.kubeflow.org/arguments.parameters":"{\"value\": \"{{inputs.parameters.send-alerts-Output}}\"}","pipelines.kubeflow.org/component_ref":"{}","pipelines.kubeflow.org/component_spec":"{\"implementation\": {\"container\": {\"args\": [\"--value\", {\"inputValue\": \"value\"}, \"--mlpipeline-metrics\", {\"outputPath\": \"mlpipeline_metrics\"}], \"command\": [\"sh\", \"-ec\", \"program_path=$(mktemp)\\nprintf \\\"%s\\\" \\\"$0\\\" \u003e \\\"$program_path\\\"\\npython3 -u \\\"$program_path\\\" \\\"$@\\\"\\n\", \"def _make_parent_dirs_and_return_path(file_path: str):\\n    import os\\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n    return file_path\\n\\ndef RDS_metrics(value,\\n  # Note when the `create_component_from_func` method converts the function to a component, the function parameter \\\"mlpipeline_metrics_path\\\" becomes an output with name \\\"mlpipeline_metrics\\\" which is the correct name for metrics output.\\n  mlpipeline_metrics_path\\n):\\n    import json\\n\\n#     import boto3\\n#     import datetime\\n#     import pandas as pd\\n#     from io import StringIO\\n#     import warnings\\n#     import numpy as np\\n#     client = boto3.client('s3')\\n#     bucket_name = 'lamm-prod-ml'\\n#     object_key = 'model_results/DB_Metrics.csv'\\n#     csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\\n#     body = csv_obj['Body']\\n#     csv_string = body.read().decode('utf-8')\\n#     metric_df = pd.read_csv(StringIO(csv_string))\\n\\n    f_count=0\\n\\n#     r_count=0\\n#     g_count=0\\n#     f_count=0\\n\\n    Accuracy = 2 # Enter total value of instance for which Pipeline is run\\n    r_count=value.count('Red Alert')\\n    g_count=value.count('Green Alert')\\n    f_count= Accuracy-(g_count+r_count)\\n\\n    # if value == \\\"Red Alert\\\":\\n    #     r_count+=1\\n    # elif value == \\\"Green Alert\\\":\\n    #     g_count+=1\\n    # else:\\n    #     f_count+=1\\n\\n    print(\\\"Red Alert Count\\\",r_count)\\n    print(\\\"Green Alert Count\\\",g_count)\\n    print(\\\"Failed Pod Count\\\",f_count)\\n\\n    Precision = r_count\\n    # Accuracy = 2\\n    Recall = g_count\\n    F1_score = f_count\\n    # model_name = 'Green'\\n\\n    metrics = {\\n        'metrics': [\\n            {\\n                'name': 'Red_Alerts',\\n                'numberValue': Precision,\\n                'format': 'RAW',\\n            },\\n            {\\n                'name': 'Total_Instances',\\n                'numberValue': Accuracy,\\n                'format': 'RAW',\\n            },\\n            {\\n                'name': 'Green_Alerts',\\n                'numberValue': Recall,\\n                'format': 'RAW',\\n            },\\n            {\\n                'name': 'Failed_Pods',\\n                'numberValue': F1_score,\\n                'format': 'RAW',\\n            },\\n\\n    #             {\\n    #                 'name': 'Alert Type',\\n    #                 'stringValue': model_name,\\n    #                 'format': 'RAW'\\n\\n    #             },\\n            ]\\n        }\\n    with open(mlpipeline_metrics_path, 'w') as f:\\n        json.dump(metrics, f)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog='RDS metrics', description='')\\n_parser.add_argument(\\\"--value\\\", dest=\\\"value\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--mlpipeline-metrics\\\", dest=\\\"mlpipeline_metrics_path\\\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parsed_args = vars(_parser.parse_args())\\n\\n_outputs = RDS_metrics(**_parsed_args)\\n\"], \"image\": \"python:3.7\"}}, \"inputs\": [{\"name\": \"value\"}], \"name\": \"RDS metrics\", \"outputs\": [{\"name\": \"mlpipeline_metrics\", \"type\": \"Metrics\"}]}","pipelines.kubeflow.org/max_cache_staleness":"P0D"},"labels":{"pipelines.kubeflow.org/enable_caching":"true","pipelines.kubeflow.org/kfp_sdk_version":"1.8.13","pipelines.kubeflow.org/pipeline-sdk-type":"kfp"}},"container":{"name":"","image":"python:3.7","command":["sh","-ec","program_path=$(mktemp)\nprintf \"%s\" \"$0\" \u003e \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n","def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\ndef RDS_metrics(value,\n  # Note when the `create_component_from_func` method converts the function to a component, the function parameter \"mlpipeline_metrics_path\" becomes an output with name \"mlpipeline_metrics\" which is the correct name for metrics output.\n  mlpipeline_metrics_path\n):\n    import json\n\n#     import boto3\n#     import datetime\n#     import pandas as pd\n#     from io import StringIO\n#     import warnings\n#     import numpy as np\n#     client = boto3.client('s3')\n#     bucket_name = 'lamm-prod-ml'\n#     object_key = 'model_results/DB_Metrics.csv'\n#     csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n#     body = csv_obj['Body']\n#     csv_string = body.read().decode('utf-8')\n#     metric_df = pd.read_csv(StringIO(csv_string))\n\n    f_count=0\n\n#     r_count=0\n#     g_count=0\n#     f_count=0\n\n    Accuracy = 2 # Enter total value of instance for which Pipeline is run\n    r_count=value.count('Red Alert')\n    g_count=value.count('Green Alert')\n    f_count= Accuracy-(g_count+r_count)\n\n    # if value == \"Red Alert\":\n    #     r_count+=1\n    # elif value == \"Green Alert\":\n    #     g_count+=1\n    # else:\n    #     f_count+=1\n\n    print(\"Red Alert Count\",r_count)\n    print(\"Green Alert Count\",g_count)\n    print(\"Failed Pod Count\",f_count)\n\n    Precision = r_count\n    # Accuracy = 2\n    Recall = g_count\n    F1_score = f_count\n    # model_name = 'Green'\n\n    metrics = {\n        'metrics': [\n            {\n                'name': 'Red_Alerts',\n                'numberValue': Precision,\n                'format': 'RAW',\n            },\n            {\n                'name': 'Total_Instances',\n                'numberValue': Accuracy,\n                'format': 'RAW',\n            },\n            {\n                'name': 'Green_Alerts',\n                'numberValue': Recall,\n                'format': 'RAW',\n            },\n            {\n                'name': 'Failed_Pods',\n                'numberValue': F1_score,\n                'format': 'RAW',\n            },\n\n    #             {\n    #                 'name': 'Alert Type',\n    #                 'stringValue': model_name,\n    #                 'format': 'RAW'\n\n    #             },\n            ]\n        }\n    with open(mlpipeline_metrics_path, 'w') as f:\n        json.dump(metrics, f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='RDS metrics', description='')\n_parser.add_argument(\"--value\", dest=\"value\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-metrics\", dest=\"mlpipeline_metrics_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs = RDS_metrics(**_parsed_args)\n"],"args":["--value","{{inputs.parameters.send-alerts-Output}}","--mlpipeline-metrics","/tmp/outputs/mlpipeline_metrics/data"],"resources":{}}},{"name":"red-alert","inputs":{},"outputs":{"artifacts":[{"name":"red-alert-Output","path":"/tmp/outputs/Output/data"}]},"metadata":{"annotations":{"pipelines.kubeflow.org/arguments.parameters":"{\"text\": \"Red Alert\"}","pipelines.kubeflow.org/component_ref":"{}","pipelines.kubeflow.org/component_spec":"{\"implementation\": {\"container\": {\"args\": [\"--text\", {\"inputValue\": \"text\"}, \"----output-paths\", {\"outputPath\": \"Output\"}], \"command\": [\"sh\", \"-ec\", \"program_path=$(mktemp)\\nprintf \\\"%s\\\" \\\"$0\\\" \u003e \\\"$program_path\\\"\\npython3 -u \\\"$program_path\\\" \\\"$@\\\"\\n\", \"def RED_ALERT(text):\\n    print('echo \\\"Results:{} \\\"'.format(text))\\n    print(\\\"text\\\",text)\\n\\n    return text\\n\\ndef _serialize_str(str_value: str) -\u003e str:\\n    if not isinstance(str_value, str):\\n        raise TypeError('Value \\\"{}\\\" has type \\\"{}\\\" instead of str.'.format(\\n            str(str_value), str(type(str_value))))\\n    return str_value\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog='RED ALERT', description='')\\n_parser.add_argument(\\\"--text\\\", dest=\\\"text\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"----output-paths\\\", dest=\\\"_output_paths\\\", type=str, nargs=1)\\n_parsed_args = vars(_parser.parse_args())\\n_output_files = _parsed_args.pop(\\\"_output_paths\\\", [])\\n\\n_outputs = RED_ALERT(**_parsed_args)\\n\\n_outputs = [_outputs]\\n\\n_output_serializers = [\\n    _serialize_str,\\n\\n]\\n\\nimport os\\nfor idx, output_file in enumerate(_output_files):\\n    try:\\n        os.makedirs(os.path.dirname(output_file))\\n    except OSError:\\n        pass\\n    with open(output_file, 'w') as f:\\n        f.write(_output_serializers[idx](_outputs[idx]))\\n\"], \"image\": \"python:3.7\"}}, \"inputs\": [{\"name\": \"text\", \"type\": \"String\"}], \"name\": \"RED ALERT\", \"outputs\": [{\"name\": \"Output\", \"type\": \"String\"}]}","pipelines.kubeflow.org/max_cache_staleness":"P0D"},"labels":{"pipelines.kubeflow.org/enable_caching":"true","pipelines.kubeflow.org/kfp_sdk_version":"1.8.13","pipelines.kubeflow.org/pipeline-sdk-type":"kfp"}},"container":{"name":"","image":"python:3.7","command":["sh","-ec","program_path=$(mktemp)\nprintf \"%s\" \"$0\" \u003e \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n","def RED_ALERT(text):\n    print('echo \"Results:{} \"'.format(text))\n    print(\"text\",text)\n\n    return text\n\ndef _serialize_str(str_value: str) -\u003e str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value), str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='RED ALERT', description='')\n_parser.add_argument(\"--text\", dest=\"text\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = RED_ALERT(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],"args":["--text","Red Alert","----output-paths","/tmp/outputs/Output/data"],"resources":{}}},{"name":"send-alerts","inputs":{"parameters":[{"name":"calculating-threshold-Output"},{"name":"loop-item-param-1-subvar-region"},{"name":"loop-item-param-1-subvar-resource_identifier"},{"name":"loop-item-param-1-subvar-resource_type"}],"artifacts":[{"name":"forecast-model-result_new_forecast_df","path":"/tmp/inputs/result_new_forecast_df/data"},{"name":"forecast-model-results","path":"/tmp/inputs/results/data"}]},"outputs":{"parameters":[{"name":"send-alerts-Output","valueFrom":{"path":"/tmp/outputs/Output/data"}}],"artifacts":[{"name":"send-alerts-Output","path":"/tmp/outputs/Output/data"}]},"metadata":{"annotations":{"pipelines.kubeflow.org/arguments.parameters":"{\"THRESHOLD\": \"{{inputs.parameters.calculating-threshold-Output}}\", \"region\": \"{{inputs.parameters.loop-item-param-1-subvar-region}}\", \"resource_identifier\": \"{{inputs.parameters.loop-item-param-1-subvar-resource_type}}\", \"resource_type\": \"{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}\"}","pipelines.kubeflow.org/component_ref":"{}","pipelines.kubeflow.org/component_spec":"{\"implementation\": {\"container\": {\"args\": [\"--results\", {\"inputPath\": \"results\"}, \"--result-new-forecast-df\", {\"inputPath\": \"result_new_forecast_df\"}, \"--THRESHOLD\", {\"inputValue\": \"THRESHOLD\"}, \"--region\", {\"inputValue\": \"region\"}, \"--resource-identifier\", {\"inputValue\": \"resource_identifier\"}, \"--resource-type\", {\"inputValue\": \"resource_type\"}, \"----output-paths\", {\"outputPath\": \"Output\"}], \"command\": [\"sh\", \"-c\", \"(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' --user) \u0026\u0026 \\\"$0\\\" \\\"$@\\\"\", \"sh\", \"-ec\", \"program_path=$(mktemp)\\nprintf \\\"%s\\\" \\\"$0\\\" \u003e \\\"$program_path\\\"\\npython3 -u \\\"$program_path\\\" \\\"$@\\\"\\n\", \"def Send_Alerts(results,result_new_forecast_df,THRESHOLD,region,resource_identifier,resource_type):\\n    # new_forecast_df,Next_Date,THRESHOLD,region,resource_identifier,resource_type\\n    failed_rds_instances= []\\n    value =''\\n    # try:\\n    import datetime\\n    import pandas as pd\\n    results = pd.read_csv(results)\\n    new_forecast_df = pd.read_csv(result_new_forecast_df)\\n\\n    print(\\\"*******\\\")\\n    if 'Unnamed: 0' in new_forecast_df:\\n        new_forecast_df = new_forecast_df.drop(['Unnamed: 0'], axis = 1)\\n        new_forecast_df = new_forecast_df.reset_index(drop=True)\\n\\n    else:\\n        pass\\n\\n    print(\\\"*******\\\")\\n    Next_Date = results['Incremented Date']\\n\\n    forecasted_value = new_forecast_df\\n    print(\\\"Forecasted Value\\\",forecasted_value)\\n    print(\\\"Start\\\")\\n    forecasted_value.columns = ['Date','Forecasted_Value']\\n    print(\\\"End\\\")\\n    print('forecasted dataframe', forecasted_value)\\n    fvalue = forecasted_value.loc[0, 'Forecasted_Value']\\n    fvalue = int(fvalue)\\n    THRESHOLD = int(THRESHOLD)\\n    print(\\\"Forecasted value of CPU Utilization is::\\\", fvalue)\\n    print(\\\"Threshold is::\\\", THRESHOLD)\\n    flag = 0\\n    if fvalue \u003e THRESHOLD:\\n        flag = 1\\n\\n    import smtplib\\n    import pandas as pd\\n    resulted_df = forecasted_value\\n    resulted_df.columns = ['Date','Forecasted_Value']\\n    # Set up the SMTP server\\n    server = smtplib.SMTP('mailprod-observability-uswest2.cloudtrust.rocks', 25)\\n\\n    from_address = 'admin@informaticacloud.com'\\n    to_address   = 'samale@informatica.com'\\n    # to_address = ['sbaig@informatica.com','samale@informatica.com','apanjwani@informatica.com','saka@informatica.com']\\n    #to_address = ['sbaig@informatica.com','samale@informatica.com','apanjwani@informatica.com','amchandrasehar@informatica.com','saka@informatica.com']\\n\\n    date = resulted_df.loc[0, 'Date']\\n    fcount =  int(resulted_df.loc[0, 'Forecasted_Value'])\\n\\n    import datetime\\n    # next_date_time = datetime.datetime.utcnow() + datetime.timedelta(minutes=30)\\n    # next_time = next_date_time.strftime('%Y-%m-%d %H:%M:%S')\\n    # print(\\\"Next 30 min\\\",next_time)\\n\\n    next_time = Next_Date\\n    print(\\\"Next 30 min\\\",next_time)\\n    flag = int(flag)\\n    subject = ''\\n    if flag == 1 and THRESHOLD != 0:\\n        subject = f'Threshold Exceeded for CPU Utilization of Region:{region},Resource type: {resource_type} and resource identifier :{resource_identifier}'\\n        body = f'''\\n        Heyy,\\n\\n        Forecasted CPU Utilization % of RDS {resource_identifier} of region {region} has crossed the threshold has crossed the threshold and Forecasted CPU Utilization count for next 30 minutes(till :{next_time}) would go as high as: {fvalue}.\\n        FYI - Dynamic threshold calculated is {THRESHOLD}.\\n\\n        Please take precausionary measures for the RDS {resource_identifier} of region {region}.\\n\\n        Warm Regards,\\n        MLOps team\\n        '''\\n\\n        #body = f\\\"DB connection count for the date {date} would go as high as {fcount}\\\"\\n        msg = f'Subject: {subject}\\\\n\\\\n{body}'\\n        server.sendmail(from_address, to_address, msg)\\n        # Disconnect from the server\\n        server.quit()\\n\\n    if flag == 1:\\n        value = \\\"Red Alert\\\"\\n    else:\\n        value = \\\"Green Alert\\\"\\n\\n    print(\\\"Value:\\\",value)\\n    print(f\\\"Successfully executed Alert process for {resource_type} and {resource_identifier}\\\")\\n\\n    resource= resource_type\\n\\n    if value != \\\"Red Alert\\\" and value != \\\"Green Alert\\\":\\n        failed_rds_instances.append(resource)\\n        print(f'Failed Instance is Resource Type: {resource_type} and Resouce Identifier : {resource_identifier} of region {region}')\\n\\n    no_failed_rds=len(failed_rds_instances)\\n\\n    if no_failed_rds == 0:\\n        print(\\\"Execution of all RDS instances Successfully completed\\\")\\n    else:\\n        print(\\\"Failed number of Rds Instances:\\\", no_failed_rds)\\n        print(\\\"List of Failed RDS Instances:\\\", failed_rds_instances)\\n        value = 'Failed'\\n\\n    return value\\n\\ndef _serialize_str(str_value: str) -\u003e str:\\n    if not isinstance(str_value, str):\\n        raise TypeError('Value \\\"{}\\\" has type \\\"{}\\\" instead of str.'.format(\\n            str(str_value), str(type(str_value))))\\n    return str_value\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog='Send Alerts', description='')\\n_parser.add_argument(\\\"--results\\\", dest=\\\"results\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--result-new-forecast-df\\\", dest=\\\"result_new_forecast_df\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--THRESHOLD\\\", dest=\\\"THRESHOLD\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--region\\\", dest=\\\"region\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--resource-identifier\\\", dest=\\\"resource_identifier\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--resource-type\\\", dest=\\\"resource_type\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"----output-paths\\\", dest=\\\"_output_paths\\\", type=str, nargs=1)\\n_parsed_args = vars(_parser.parse_args())\\n_output_files = _parsed_args.pop(\\\"_output_paths\\\", [])\\n\\n_outputs = Send_Alerts(**_parsed_args)\\n\\n_outputs = [_outputs]\\n\\n_output_serializers = [\\n    _serialize_str,\\n\\n]\\n\\nimport os\\nfor idx, output_file in enumerate(_output_files):\\n    try:\\n        os.makedirs(os.path.dirname(output_file))\\n    except OSError:\\n        pass\\n    with open(output_file, 'w') as f:\\n        f.write(_output_serializers[idx](_outputs[idx]))\\n\"], \"image\": \"python:3.7\"}}, \"inputs\": [{\"name\": \"results\"}, {\"name\": \"result_new_forecast_df\"}, {\"name\": \"THRESHOLD\"}, {\"name\": \"region\"}, {\"name\": \"resource_identifier\"}, {\"name\": \"resource_type\"}], \"name\": \"Send Alerts\", \"outputs\": [{\"name\": \"Output\", \"type\": \"String\"}]}","pipelines.kubeflow.org/max_cache_staleness":"P0D"},"labels":{"pipelines.kubeflow.org/enable_caching":"true","pipelines.kubeflow.org/kfp_sdk_version":"1.8.13","pipelines.kubeflow.org/pipeline-sdk-type":"kfp"}},"container":{"name":"","image":"python:3.7","command":["sh","-c","(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' --user) \u0026\u0026 \"$0\" \"$@\"","sh","-ec","program_path=$(mktemp)\nprintf \"%s\" \"$0\" \u003e \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n","def Send_Alerts(results,result_new_forecast_df,THRESHOLD,region,resource_identifier,resource_type):\n    # new_forecast_df,Next_Date,THRESHOLD,region,resource_identifier,resource_type\n    failed_rds_instances= []\n    value =''\n    # try:\n    import datetime\n    import pandas as pd\n    results = pd.read_csv(results)\n    new_forecast_df = pd.read_csv(result_new_forecast_df)\n\n    print(\"*******\")\n    if 'Unnamed: 0' in new_forecast_df:\n        new_forecast_df = new_forecast_df.drop(['Unnamed: 0'], axis = 1)\n        new_forecast_df = new_forecast_df.reset_index(drop=True)\n\n    else:\n        pass\n\n    print(\"*******\")\n    Next_Date = results['Incremented Date']\n\n    forecasted_value = new_forecast_df\n    print(\"Forecasted Value\",forecasted_value)\n    print(\"Start\")\n    forecasted_value.columns = ['Date','Forecasted_Value']\n    print(\"End\")\n    print('forecasted dataframe', forecasted_value)\n    fvalue = forecasted_value.loc[0, 'Forecasted_Value']\n    fvalue = int(fvalue)\n    THRESHOLD = int(THRESHOLD)\n    print(\"Forecasted value of CPU Utilization is::\", fvalue)\n    print(\"Threshold is::\", THRESHOLD)\n    flag = 0\n    if fvalue \u003e THRESHOLD:\n        flag = 1\n\n    import smtplib\n    import pandas as pd\n    resulted_df = forecasted_value\n    resulted_df.columns = ['Date','Forecasted_Value']\n    # Set up the SMTP server\n    server = smtplib.SMTP('mailprod-observability-uswest2.cloudtrust.rocks', 25)\n\n    from_address = 'admin@informaticacloud.com'\n    to_address   = 'samale@informatica.com'\n    # to_address = ['sbaig@informatica.com','samale@informatica.com','apanjwani@informatica.com','saka@informatica.com']\n    #to_address = ['sbaig@informatica.com','samale@informatica.com','apanjwani@informatica.com','amchandrasehar@informatica.com','saka@informatica.com']\n\n    date = resulted_df.loc[0, 'Date']\n    fcount =  int(resulted_df.loc[0, 'Forecasted_Value'])\n\n    import datetime\n    # next_date_time = datetime.datetime.utcnow() + datetime.timedelta(minutes=30)\n    # next_time = next_date_time.strftime('%Y-%m-%d %H:%M:%S')\n    # print(\"Next 30 min\",next_time)\n\n    next_time = Next_Date\n    print(\"Next 30 min\",next_time)\n    flag = int(flag)\n    subject = ''\n    if flag == 1 and THRESHOLD != 0:\n        subject = f'Threshold Exceeded for CPU Utilization of Region:{region},Resource type: {resource_type} and resource identifier :{resource_identifier}'\n        body = f'''\n        Heyy,\n\n        Forecasted CPU Utilization % of RDS {resource_identifier} of region {region} has crossed the threshold has crossed the threshold and Forecasted CPU Utilization count for next 30 minutes(till :{next_time}) would go as high as: {fvalue}.\n        FYI - Dynamic threshold calculated is {THRESHOLD}.\n\n        Please take precausionary measures for the RDS {resource_identifier} of region {region}.\n\n        Warm Regards,\n        MLOps team\n        '''\n\n        #body = f\"DB connection count for the date {date} would go as high as {fcount}\"\n        msg = f'Subject: {subject}\\n\\n{body}'\n        server.sendmail(from_address, to_address, msg)\n        # Disconnect from the server\n        server.quit()\n\n    if flag == 1:\n        value = \"Red Alert\"\n    else:\n        value = \"Green Alert\"\n\n    print(\"Value:\",value)\n    print(f\"Successfully executed Alert process for {resource_type} and {resource_identifier}\")\n\n    resource= resource_type\n\n    if value != \"Red Alert\" and value != \"Green Alert\":\n        failed_rds_instances.append(resource)\n        print(f'Failed Instance is Resource Type: {resource_type} and Resouce Identifier : {resource_identifier} of region {region}')\n\n    no_failed_rds=len(failed_rds_instances)\n\n    if no_failed_rds == 0:\n        print(\"Execution of all RDS instances Successfully completed\")\n    else:\n        print(\"Failed number of Rds Instances:\", no_failed_rds)\n        print(\"List of Failed RDS Instances:\", failed_rds_instances)\n        value = 'Failed'\n\n    return value\n\ndef _serialize_str(str_value: str) -\u003e str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value), str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Send Alerts', description='')\n_parser.add_argument(\"--results\", dest=\"results\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--result-new-forecast-df\", dest=\"result_new_forecast_df\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--THRESHOLD\", dest=\"THRESHOLD\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--region\", dest=\"region\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--resource-identifier\", dest=\"resource_identifier\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--resource-type\", dest=\"resource_type\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = Send_Alerts(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],"args":["--results","/tmp/inputs/results/data","--result-new-forecast-df","/tmp/inputs/result_new_forecast_df/data","--THRESHOLD","{{inputs.parameters.calculating-threshold-Output}}","--region","{{inputs.parameters.loop-item-param-1-subvar-region}}","--resource-identifier","{{inputs.parameters.loop-item-param-1-subvar-resource_type}}","--resource-type","{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}","----output-paths","/tmp/outputs/Output/data"],"resources":{}}},{"name":"storing-forecasted-values","inputs":{"parameters":[{"name":"calculating-threshold-Output"},{"name":"loop-item-param-1-subvar-region"},{"name":"loop-item-param-1-subvar-resource_identifier"},{"name":"loop-item-param-1-subvar-resource_type"}],"artifacts":[{"name":"forecast-model-result_new_forecast_df","path":"/tmp/inputs/result_new_forecast_df/data"},{"name":"forecast-model-results","path":"/tmp/inputs/results/data"}]},"outputs":{},"metadata":{"annotations":{"pipelines.kubeflow.org/arguments.parameters":"{\"THRESHOLD\": \"{{inputs.parameters.calculating-threshold-Output}}\", \"region\": \"{{inputs.parameters.loop-item-param-1-subvar-region}}\", \"resource_identifier\": \"{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}\", \"resource_type\": \"{{inputs.parameters.loop-item-param-1-subvar-resource_type}}\"}","pipelines.kubeflow.org/component_ref":"{}","pipelines.kubeflow.org/component_spec":"{\"implementation\": {\"container\": {\"args\": [\"--results\", {\"inputPath\": \"results\"}, \"--result-new-forecast-df\", {\"inputPath\": \"result_new_forecast_df\"}, \"--THRESHOLD\", {\"inputValue\": \"THRESHOLD\"}, \"--region\", {\"inputValue\": \"region\"}, \"--resource-type\", {\"inputValue\": \"resource_type\"}, \"--resource-identifier\", {\"inputValue\": \"resource_identifier\"}], \"command\": [\"sh\", \"-c\", \"(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' --user) \u0026\u0026 \\\"$0\\\" \\\"$@\\\"\", \"sh\", \"-ec\", \"program_path=$(mktemp)\\nprintf \\\"%s\\\" \\\"$0\\\" \u003e \\\"$program_path\\\"\\npython3 -u \\\"$program_path\\\" \\\"$@\\\"\\n\", \"def Storing_Forecasted_Values(results,result_new_forecast_df,THRESHOLD,region,resource_type,resource_identifier):    \\n    # incremented_date,today_date,THRESHOLD,f,today_actual,region,resource_type,resource_identifier\\n    import datetime\\n    import pandas as pd\\n    import json\\n    import boto3\\n    import sys\\n    from io import StringIO\\n\\n    results = pd.read_csv(results)\\n\\n    incremented_date = results['Incremented Date'][0]\\n    print(\\\"incremented date:\\\",incremented_date)\\n    today_date = results['Today Date'][0]\\n    print(\\\"today_date:\\\",today_date)\\n    today_actual = results['Today Actual'][0]\\n    print(\\\"today_actual:\\\",today_actual)\\n    next_day_forecast = results['Next Day Forecast'][0]\\n    print(\\\"Next Day Forecast:\\\",next_day_forecast)\\n\\n    new_forecast_df = pd.read_csv(result_new_forecast_df)\\n\\n    print(\\\"Incremented Date\\\",incremented_date)\\n    print(\\\"Today Date\\\",today_date)\\n    print(\\\"Todays Actual Date\\\",today_actual)\\n    print(\\\"forecast value\\\",next_day_forecast)\\n\\n    client = boto3.client('s3')\\n    bucket_name = 'lamm-prod-ml'\\n    object_key = 'Monitoring/CPU Utilization Monitoring/'+region+'_CPU_Forecast_Monitoring.csv'\\n    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\\n    body = csv_obj['Body']\\n    csv_string = body.read().decode('utf-8')\\n    monitor_df = pd.read_csv(StringIO(csv_string))\\n    print(\\\"Monitor df:\\\",monitor_df.tail(5))\\n\\n    print(\\\"$$$$$$$$$$$$$$$$\\\")\\n    # Next_Date = today_date + datetime.timedelta(days=1)\\n    # print(\\\"Next Date\\\",Next_Date)\\n    Next_Date = incremented_date\\n    print(\\\"Next Date Time\\\",Next_Date)\\n    # Previous_Date = today_date - datetime.timedelta(days=1)\\n\\n    for i, values in monitor_df['date'].iteritems():\\n        print(\\\"********************************************\\\")\\n        print(\\\"Next Date:\\\",Next_Date)\\n        x=pd.to_datetime(values)\\n        print(\\\"Values in date index\\\",x)\\n        print(\\\"********************************************\\\")\\n        if x == Next_Date:\\n            print(\\\"Date Matched\\\")\\n            print(\\\"Forecasted Value\\\",next_day_forecast)\\n            monitor_df[forecast_term].loc[i] = next_day_forecast\\n            monitor_df[threshold_term].loc[i] = THRESHOLD\\n            print(\\\"Forecasted CPU Utilization and Threshold Updated in Monitoring file\\\")\\n        else:\\n            # print(\\\"Dates didnt Matched\\\")\\n            # print(\\\"Tomorrow's Date Not updated in Monitoring File. Please Update \\\")\\n            pass\\n\\n    for i, values in monitor_df['date'].iteritems():\\n        y = pd.to_datetime(values)\\n        print(y)\\n        if y == today_date:\\n            print(\\\"Next Date\\\",today_date)\\n            print(\\\"Values in date index\\\",pd.to_datetime(values))\\n            print(\\\"Dates matched for updating Actual Values\\\")\\n            monitor_df[resource_identifier].loc[i] = today_actual\\n            print(\\\"Yesterdays Actual CPU Utilization % Updated in Monitoring file\\\")\\n        else:\\n            pass\\n\\n    print(\\\"*******\\\")\\n    if 'Unnamed: 0' in monitor_df:\\n        monitor_df = monitor_df.drop(['Unnamed: 0'], axis = 1)\\n        monitor_df = monitor_df.reset_index(drop=True)\\n\\n    else:\\n        pass\\n\\n    print(\\\"*******\\\")\\n\\n#         rows = monitor_df.shape[0]\\n\\n#         actual_list = list()\\n#         forecast_list = list()\\n\\n# #         print(forecast_term)\\n#         for i in np.arange(rows):\\n#             # print(pd.notna(monitor_updated.at[i,'actual']))\\n#             if pd.notna(monitor_df.at[i,resource_identifier]) \u0026 pd.notna(monitor_df.at[i,forecast_term]):\\n#                 actual_list.append(monitor_df[resource_identifier][i])\\n#                 forecast_list.append(monitor_df[forecast_term][i])\\n\\n    csv_buffer = StringIO()\\n    monitor_df.to_csv(csv_buffer)\\n    s3_resource = boto3.resource('s3')\\n    s3_resource.Object(bucket_name, 'Monitoring/CPU Utilization Monitoring/'+region+'_CPU_Forecast_Monitoring.csv').put(Body=csv_buffer.getvalue())\\n    print('Performance Monitor file has been uploaded')\\n\\n#         try:\\n#             weekly_mape = []\\n#             if len(actual_list) \u003e= 7:\\n#                 actual_series = pd.Series(actual_list[-7:])\\n#                 forecast_series = pd.Series(forecast_list[-7:])\\n#                 weekly_mape_value= np.mean(np.abs((actual_series[-7:] - forecast_series[-7:]) / actual_series[-7:]))*100\\n#                 weekly_mape.append(weekly_mape_value)\\n\\n#             weekly_mape_value = weekly_mape[0]\\n#             print(\\\"Weekly MAPE Value\\\",weekly_mape_value)\\n\\n#         except:\\n#             pass\\n\\n    print(f\\\"Successfully stored forecasted value for {region}, {resource_type} and {resource_identifier}\\\")\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog='Storing Forecasted Values', description='')\\n_parser.add_argument(\\\"--results\\\", dest=\\\"results\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--result-new-forecast-df\\\", dest=\\\"result_new_forecast_df\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--THRESHOLD\\\", dest=\\\"THRESHOLD\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--region\\\", dest=\\\"region\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--resource-type\\\", dest=\\\"resource_type\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"--resource-identifier\\\", dest=\\\"resource_identifier\\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parsed_args = vars(_parser.parse_args())\\n\\n_outputs = Storing_Forecasted_Values(**_parsed_args)\\n\"], \"image\": \"python:3.7\"}}, \"inputs\": [{\"name\": \"results\"}, {\"name\": \"result_new_forecast_df\"}, {\"name\": \"THRESHOLD\"}, {\"name\": \"region\"}, {\"name\": \"resource_type\"}, {\"name\": \"resource_identifier\"}], \"name\": \"Storing Forecasted Values\"}","pipelines.kubeflow.org/max_cache_staleness":"P0D"},"labels":{"pipelines.kubeflow.org/enable_caching":"true","pipelines.kubeflow.org/kfp_sdk_version":"1.8.13","pipelines.kubeflow.org/pipeline-sdk-type":"kfp"}},"container":{"name":"","image":"python:3.7","command":["sh","-c","(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'boto3' 'pandas' 'statsmodels' 'matplotlib' 'seaborn' 'prophet' 'scikit-learn' 'pmdarima' --user) \u0026\u0026 \"$0\" \"$@\"","sh","-ec","program_path=$(mktemp)\nprintf \"%s\" \"$0\" \u003e \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n","def Storing_Forecasted_Values(results,result_new_forecast_df,THRESHOLD,region,resource_type,resource_identifier):    \n    # incremented_date,today_date,THRESHOLD,f,today_actual,region,resource_type,resource_identifier\n    import datetime\n    import pandas as pd\n    import json\n    import boto3\n    import sys\n    from io import StringIO\n\n    results = pd.read_csv(results)\n\n    incremented_date = results['Incremented Date'][0]\n    print(\"incremented date:\",incremented_date)\n    today_date = results['Today Date'][0]\n    print(\"today_date:\",today_date)\n    today_actual = results['Today Actual'][0]\n    print(\"today_actual:\",today_actual)\n    next_day_forecast = results['Next Day Forecast'][0]\n    print(\"Next Day Forecast:\",next_day_forecast)\n\n    new_forecast_df = pd.read_csv(result_new_forecast_df)\n\n    print(\"Incremented Date\",incremented_date)\n    print(\"Today Date\",today_date)\n    print(\"Todays Actual Date\",today_actual)\n    print(\"forecast value\",next_day_forecast)\n\n    client = boto3.client('s3')\n    bucket_name = 'lamm-prod-ml'\n    object_key = 'Monitoring/CPU Utilization Monitoring/'+region+'_CPU_Forecast_Monitoring.csv'\n    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n    body = csv_obj['Body']\n    csv_string = body.read().decode('utf-8')\n    monitor_df = pd.read_csv(StringIO(csv_string))\n    print(\"Monitor df:\",monitor_df.tail(5))\n\n    print(\"$$$$$$$$$$$$$$$$\")\n    # Next_Date = today_date + datetime.timedelta(days=1)\n    # print(\"Next Date\",Next_Date)\n    Next_Date = incremented_date\n    print(\"Next Date Time\",Next_Date)\n    # Previous_Date = today_date - datetime.timedelta(days=1)\n\n    for i, values in monitor_df['date'].iteritems():\n        print(\"********************************************\")\n        print(\"Next Date:\",Next_Date)\n        x=pd.to_datetime(values)\n        print(\"Values in date index\",x)\n        print(\"********************************************\")\n        if x == Next_Date:\n            print(\"Date Matched\")\n            print(\"Forecasted Value\",next_day_forecast)\n            monitor_df[forecast_term].loc[i] = next_day_forecast\n            monitor_df[threshold_term].loc[i] = THRESHOLD\n            print(\"Forecasted CPU Utilization and Threshold Updated in Monitoring file\")\n        else:\n            # print(\"Dates didnt Matched\")\n            # print(\"Tomorrow's Date Not updated in Monitoring File. Please Update \")\n            pass\n\n    for i, values in monitor_df['date'].iteritems():\n        y = pd.to_datetime(values)\n        print(y)\n        if y == today_date:\n            print(\"Next Date\",today_date)\n            print(\"Values in date index\",pd.to_datetime(values))\n            print(\"Dates matched for updating Actual Values\")\n            monitor_df[resource_identifier].loc[i] = today_actual\n            print(\"Yesterdays Actual CPU Utilization % Updated in Monitoring file\")\n        else:\n            pass\n\n    print(\"*******\")\n    if 'Unnamed: 0' in monitor_df:\n        monitor_df = monitor_df.drop(['Unnamed: 0'], axis = 1)\n        monitor_df = monitor_df.reset_index(drop=True)\n\n    else:\n        pass\n\n    print(\"*******\")\n\n#         rows = monitor_df.shape[0]\n\n#         actual_list = list()\n#         forecast_list = list()\n\n# #         print(forecast_term)\n#         for i in np.arange(rows):\n#             # print(pd.notna(monitor_updated.at[i,'actual']))\n#             if pd.notna(monitor_df.at[i,resource_identifier]) \u0026 pd.notna(monitor_df.at[i,forecast_term]):\n#                 actual_list.append(monitor_df[resource_identifier][i])\n#                 forecast_list.append(monitor_df[forecast_term][i])\n\n    csv_buffer = StringIO()\n    monitor_df.to_csv(csv_buffer)\n    s3_resource = boto3.resource('s3')\n    s3_resource.Object(bucket_name, 'Monitoring/CPU Utilization Monitoring/'+region+'_CPU_Forecast_Monitoring.csv').put(Body=csv_buffer.getvalue())\n    print('Performance Monitor file has been uploaded')\n\n#         try:\n#             weekly_mape = []\n#             if len(actual_list) \u003e= 7:\n#                 actual_series = pd.Series(actual_list[-7:])\n#                 forecast_series = pd.Series(forecast_list[-7:])\n#                 weekly_mape_value= np.mean(np.abs((actual_series[-7:] - forecast_series[-7:]) / actual_series[-7:]))*100\n#                 weekly_mape.append(weekly_mape_value)\n\n#             weekly_mape_value = weekly_mape[0]\n#             print(\"Weekly MAPE Value\",weekly_mape_value)\n\n#         except:\n#             pass\n\n    print(f\"Successfully stored forecasted value for {region}, {resource_type} and {resource_identifier}\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Storing Forecasted Values', description='')\n_parser.add_argument(\"--results\", dest=\"results\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--result-new-forecast-df\", dest=\"result_new_forecast_df\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--THRESHOLD\", dest=\"THRESHOLD\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--region\", dest=\"region\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--resource-type\", dest=\"resource_type\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--resource-identifier\", dest=\"resource_identifier\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs = Storing_Forecasted_Values(**_parsed_args)\n"],"args":["--results","/tmp/inputs/results/data","--result-new-forecast-df","/tmp/inputs/result_new_forecast_df/data","--THRESHOLD","{{inputs.parameters.calculating-threshold-Output}}","--region","{{inputs.parameters.loop-item-param-1-subvar-region}}","--resource-type","{{inputs.parameters.loop-item-param-1-subvar-resource_type}}","--resource-identifier","{{inputs.parameters.loop-item-param-1-subvar-resource_identifier}}"],"resources":{}}}],"entrypoint":"db-connection-forecasting-model","arguments":{},"serviceAccountName":"pipeline-runner"},"status":{"startedAt":null,"finishedAt":null}}
